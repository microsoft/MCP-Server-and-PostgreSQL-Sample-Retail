<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "907b008a90c728fd33a1c0f66889e8e2",
  "translation_date": "2025-09-29T22:24:53+00:00",
  "source_file": "walkthrough/01-Architecture/README.md",
  "language_code": "vi"
}
-->
# C√°c Kh√°i Ni·ªám C·ªët L√µi V·ªÅ Ki·∫øn Tr√∫c

## üéØ N·ªôi Dung C·ªßa Module N√†y

Module n√†y cung c·∫•p m·ªôt c√°i nh√¨n s√¢u s·∫Øc v·ªÅ c√°c m·∫´u ki·∫øn tr√∫c m√°y ch·ªß MCP, nguy√™n t·∫Øc thi·∫øt k·∫ø c∆° s·ªü d·ªØ li·ªáu, v√† c√°c chi·∫øn l∆∞·ª£c tri·ªÉn khai k·ªπ thu·∫≠t ƒë·ªÉ x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng AI t√≠ch h·ª£p c∆° s·ªü d·ªØ li·ªáu m·∫°nh m·∫Ω v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông.

## T·ªïng Quan

Vi·ªác x√¢y d·ª±ng m·ªôt m√°y ch·ªß MCP s·∫µn s√†ng cho s·∫£n xu·∫•t v·ªõi t√≠ch h·ª£p c∆° s·ªü d·ªØ li·ªáu ƒë√≤i h·ªèi c√°c quy·∫øt ƒë·ªãnh ki·∫øn tr√∫c c·∫©n th·∫≠n. Module n√†y ph√¢n t√≠ch c√°c th√†nh ph·∫ßn ch√≠nh, m·∫´u thi·∫øt k·∫ø, v√† c√°c y·∫øu t·ªë k·ªπ thu·∫≠t gi√∫p gi·∫£i ph√°p ph√¢n t√≠ch Zava Retail c·ªßa ch√∫ng t√¥i tr·ªü n√™n m·∫°nh m·∫Ω, an to√†n v√† c√≥ kh·∫£ nƒÉng m·ªü r·ªông.

B·∫°n s·∫Ω hi·ªÉu c√°ch m·ªói l·ªõp t∆∞∆°ng t√°c, l√Ω do ch·ªçn c√°c c√¥ng ngh·ªá c·ª• th·ªÉ, v√† c√°ch √°p d·ª•ng c√°c m·∫´u n√†y v√†o tri·ªÉn khai MCP c·ªßa ri√™ng b·∫°n.

## M·ª•c Ti√™u H·ªçc T·∫≠p

Sau khi ho√†n th√†nh module n√†y, b·∫°n s·∫Ω c√≥ th·ªÉ:

- **Ph√¢n t√≠ch** ki·∫øn tr√∫c ph√¢n l·ªõp c·ªßa m√°y ch·ªß MCP v·ªõi t√≠ch h·ª£p c∆° s·ªü d·ªØ li·ªáu  
- **Hi·ªÉu** vai tr√≤ v√† tr√°ch nhi·ªám c·ªßa t·ª´ng th√†nh ph·∫ßn ki·∫øn tr√∫c  
- **Thi·∫øt k·∫ø** c√°c schema c∆° s·ªü d·ªØ li·ªáu h·ªó tr·ª£ ·ª©ng d·ª•ng MCP ƒëa kh√°ch h√†ng  
- **Tri·ªÉn khai** c√°c chi·∫øn l∆∞·ª£c qu·∫£n l√Ω k·∫øt n·ªëi v√† t√†i nguy√™n  
- **√Åp d·ª•ng** c√°c m·∫´u x·ª≠ l√Ω l·ªói v√† ghi log cho h·ªá th·ªëng s·∫£n xu·∫•t  
- **ƒê√°nh gi√°** c√°c ƒë√°nh ƒë·ªïi gi·ªØa c√°c c√°ch ti·∫øp c·∫≠n ki·∫øn tr√∫c kh√°c nhau  

## üèóÔ∏è C√°c L·ªõp Ki·∫øn Tr√∫c M√°y Ch·ªß MCP

M√°y ch·ªß MCP c·ªßa ch√∫ng t√¥i tri·ªÉn khai m·ªôt **ki·∫øn tr√∫c ph√¢n l·ªõp** nh·∫±m t√°ch bi·ªát c√°c m·ªëi quan t√¢m v√† th√∫c ƒë·∫©y kh·∫£ nƒÉng b·∫£o tr√¨:

### L·ªõp 1: L·ªõp Giao Th·ª©c (FastMCP)
**Tr√°ch nhi·ªám**: X·ª≠ l√Ω giao ti·∫øp giao th·ª©c MCP v√† ƒë·ªãnh tuy·∫øn th√¥ng ƒëi·ªáp

```python
# FastMCP server setup
from fastmcp import FastMCP

mcp = FastMCP("Zava Retail Analytics")

# Tool registration with type safety
@mcp.tool()
async def execute_sales_query(
    ctx: Context,
    postgresql_query: Annotated[str, Field(description="Well-formed PostgreSQL query")]
) -> str:
    """Execute PostgreSQL queries with Row Level Security."""
    return await query_executor.execute(postgresql_query, ctx)
```

**C√°c T√≠nh NƒÉng Ch√≠nh**:
- **Tu√¢n Th·ªß Giao Th·ª©c**: H·ªó tr·ª£ ƒë·∫ßy ƒë·ªß ƒë·∫∑c t·∫£ MCP  
- **An To√†n Ki·ªÉu D·ªØ Li·ªáu**: C√°c m√¥ h√¨nh Pydantic ƒë·ªÉ x√°c th·ª±c y√™u c·∫ßu/ph·∫£n h·ªìi  
- **H·ªó Tr·ª£ Async**: I/O kh√¥ng ch·∫∑n ƒë·ªÉ ƒë·∫°t ƒë·ªô ƒë·ªìng th·ªùi cao  
- **X·ª≠ L√Ω L·ªói**: Ph·∫£n h·ªìi l·ªói ƒë∆∞·ª£c chu·∫©n h√≥a  

### L·ªõp 2: L·ªõp Logic Kinh Doanh
**Tr√°ch nhi·ªám**: Tri·ªÉn khai c√°c quy t·∫Øc kinh doanh v√† ph·ªëi h·ª£p gi·ªØa l·ªõp giao th·ª©c v√† l·ªõp d·ªØ li·ªáu

```python
class SalesAnalyticsService:
    """Business logic for retail analytics operations."""
    
    async def get_store_performance(
        self, 
        store_id: str, 
        time_period: str
    ) -> Dict[str, Any]:
        """Calculate store performance metrics."""
        
        # Validate business rules
        if not self._validate_store_access(store_id):
            raise UnauthorizedError("Access denied for store")
        
        # Coordinate data retrieval
        sales_data = await self.db_provider.get_sales_data(store_id, time_period)
        metrics = self._calculate_metrics(sales_data)
        
        return {
            "store_id": store_id,
            "period": time_period,
            "metrics": metrics,
            "insights": self._generate_insights(metrics)
        }
```

**C√°c T√≠nh NƒÉng Ch√≠nh**:
- **Th·ª±c Thi Quy T·∫Øc Kinh Doanh**: X√°c th·ª±c truy c·∫≠p kho v√† t√≠nh to√†n v·∫πn d·ªØ li·ªáu  
- **Ph·ªëi H·ª£p D·ªãch V·ª•**: ƒêi·ªÅu ph·ªëi gi·ªØa c∆° s·ªü d·ªØ li·ªáu v√† c√°c d·ªãch v·ª• AI  
- **Chuy·ªÉn ƒê·ªïi D·ªØ Li·ªáu**: Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√¥ th√†nh th√¥ng tin kinh doanh  
- **Chi·∫øn L∆∞·ª£c B·ªô Nh·ªõ ƒê·ªám**: T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t cho c√°c truy v·∫•n th∆∞·ªùng xuy√™n  

### L·ªõp 3: L·ªõp Truy C·∫≠p D·ªØ Li·ªáu
**Tr√°ch nhi·ªám**: Qu·∫£n l√Ω k·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu, th·ª±c thi truy v·∫•n, v√† √°nh x·∫° d·ªØ li·ªáu

```python
class PostgreSQLProvider:
    """Data access layer for PostgreSQL operations."""
    
    def __init__(self, connection_config: Dict[str, Any]):
        self.connection_pool: Optional[Pool] = None
        self.config = connection_config
    
    async def execute_query(
        self, 
        query: str, 
        rls_user_id: str
    ) -> List[Dict[str, Any]]:
        """Execute query with RLS context."""
        
        async with self.connection_pool.acquire() as conn:
            # Set RLS context
            await conn.execute(
                "SELECT set_config('app.current_rls_user_id', $1, false)",
                rls_user_id
            )
            
            # Execute query with timeout
            try:
                rows = await asyncio.wait_for(
                    conn.fetch(query),
                    timeout=30.0
                )
                return [dict(row) for row in rows]
            except asyncio.TimeoutError:
                raise QueryTimeoutError("Query execution exceeded timeout")
```

**C√°c T√≠nh NƒÉng Ch√≠nh**:
- **Qu·∫£n L√Ω K·∫øt N·ªëi**: Qu·∫£n l√Ω t√†i nguy√™n hi·ªáu qu·∫£  
- **Qu·∫£n L√Ω Giao D·ªãch**: Tu√¢n th·ªß ACID v√† x·ª≠ l√Ω rollback  
- **T·ªëi ∆Øu H√≥a Truy V·∫•n**: Gi√°m s√°t v√† t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t  
- **T√≠ch H·ª£p RLS**: Qu·∫£n l√Ω ng·ªØ c·∫£nh b·∫£o m·∫≠t c·∫•p h√†ng  

### L·ªõp 4: L·ªõp H·∫° T·∫ßng
**Tr√°ch nhi·ªám**: X·ª≠ l√Ω c√°c m·ªëi quan t√¢m chung nh∆∞ ghi log, gi√°m s√°t, v√† c·∫•u h√¨nh

```python
class InfrastructureManager:
    """Infrastructure concerns management."""
    
    def __init__(self):
        self.logger = self._setup_logging()
        self.metrics = self._setup_metrics()
        self.config = self._load_configuration()
    
    def _setup_logging(self) -> Logger:
        """Configure structured logging."""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(),
                logging.FileHandler('mcp_server.log')
            ]
        )
        return logging.getLogger(__name__)
    
    async def track_query_execution(
        self, 
        query_type: str, 
        duration: float, 
        success: bool
    ):
        """Track query performance metrics."""
        self.metrics.counter('query_total').labels(
            type=query_type,
            status='success' if success else 'error'
        ).inc()
        
        self.metrics.histogram('query_duration').labels(
            type=query_type
        ).observe(duration)
```

## üóÑÔ∏è C√°c M·∫´u Thi·∫øt K·∫ø C∆° S·ªü D·ªØ Li·ªáu

Schema PostgreSQL c·ªßa ch√∫ng t√¥i tri·ªÉn khai m·ªôt s·ªë m·∫´u ch√≠nh cho c√°c ·ª©ng d·ª•ng MCP ƒëa kh√°ch h√†ng:

### 1. Thi·∫øt K·∫ø Schema ƒêa Kh√°ch H√†ng

```sql
-- Core retail entities with store-based partitioning
CREATE TABLE retail.stores (
    store_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(100) NOT NULL,
    location VARCHAR(200) NOT NULL,
    manager_id UUID NOT NULL,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE retail.customers (
    customer_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    store_id UUID REFERENCES retail.stores(store_id),
    first_name VARCHAR(50) NOT NULL,
    last_name VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE,
    created_at TIMESTAMP DEFAULT NOW()
);

CREATE TABLE retail.orders (
    order_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    customer_id UUID REFERENCES retail.customers(customer_id),
    store_id UUID REFERENCES retail.stores(store_id),
    order_date TIMESTAMP DEFAULT NOW(),
    total_amount DECIMAL(10,2) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending'
);
```

**Nguy√™n T·∫Øc Thi·∫øt K·∫ø**:
- **T√≠nh Nh·∫•t Qu√°n Kh√≥a Ngo·∫°i**: ƒê·∫£m b·∫£o t√≠nh to√†n v·∫πn d·ªØ li·ªáu gi·ªØa c√°c b·∫£ng  
- **Truy·ªÅn ID Kho**: M·ªói b·∫£ng giao d·ªãch ƒë·ªÅu bao g·ªìm store_id  
- **Kh√≥a Ch√≠nh UUID**: C√°c ƒë·ªãnh danh duy nh·∫•t to√†n c·∫ßu cho h·ªá th·ªëng ph√¢n t√°n  
- **Theo D√µi D·∫•u Th·ªùi Gian**: L∆∞u v·∫øt ki·ªÉm tra cho t·∫•t c·∫£ c√°c thay ƒë·ªïi d·ªØ li·ªáu  

### 2. Tri·ªÉn Khai B·∫£o M·∫≠t C·∫•p H√†ng (RLS)

```sql
-- Enable RLS on multi-tenant tables
ALTER TABLE retail.customers ENABLE ROW LEVEL SECURITY;
ALTER TABLE retail.orders ENABLE ROW LEVEL SECURITY;
ALTER TABLE retail.order_items ENABLE ROW LEVEL SECURITY;

-- Store manager can only see their store's data
CREATE POLICY store_manager_customers ON retail.customers
    FOR ALL TO store_managers
    USING (store_id = get_current_user_store());

CREATE POLICY store_manager_orders ON retail.orders
    FOR ALL TO store_managers
    USING (store_id = get_current_user_store());

-- Regional managers see multiple stores
CREATE POLICY regional_manager_orders ON retail.orders
    FOR ALL TO regional_managers
    USING (store_id = ANY(get_user_store_list()));

-- Support function for RLS context
CREATE OR REPLACE FUNCTION get_current_user_store()
RETURNS UUID AS $$
BEGIN
    RETURN current_setting('app.current_rls_user_id')::UUID;
EXCEPTION WHEN OTHERS THEN
    RETURN '00000000-0000-0000-0000-000000000000'::UUID;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

**L·ª£i √çch RLS**:
- **L·ªçc T·ª± ƒê·ªông**: C∆° s·ªü d·ªØ li·ªáu th·ª±c thi c√°ch ly d·ªØ li·ªáu  
- **ƒê∆°n Gi·∫£n H√≥a ·ª®ng D·ª•ng**: Kh√¥ng c·∫ßn c√°c c√¢u WHERE ph·ª©c t·∫°p  
- **B·∫£o M·∫≠t M·∫∑c ƒê·ªãnh**: Kh√¥ng th·ªÉ v√¥ t√¨nh truy c·∫≠p d·ªØ li·ªáu sai  
- **Tu√¢n Th·ªß Ki·ªÉm To√°n**: Ranh gi·ªõi truy c·∫≠p d·ªØ li·ªáu r√µ r√†ng  

### 3. Schema T√¨m Ki·∫øm Vector

```sql
-- Product embeddings for semantic search
CREATE TABLE retail.product_description_embeddings (
    product_id UUID PRIMARY KEY REFERENCES retail.products(product_id),
    description_embedding vector(1536),
    last_updated TIMESTAMP DEFAULT NOW()
);

-- Optimize vector similarity search
CREATE INDEX idx_product_embeddings_vector 
ON retail.product_description_embeddings 
USING ivfflat (description_embedding vector_cosine_ops);

-- Semantic search function
CREATE OR REPLACE FUNCTION search_products_by_description(
    query_embedding vector(1536),
    similarity_threshold FLOAT DEFAULT 0.7,
    max_results INTEGER DEFAULT 20
)
RETURNS TABLE(
    product_id UUID,
    name VARCHAR,
    description TEXT,
    similarity_score FLOAT
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        p.product_id,
        p.name,
        p.description,
        (1 - (pde.description_embedding <=> query_embedding)) AS similarity_score
    FROM retail.products p
    JOIN retail.product_description_embeddings pde ON p.product_id = pde.product_id
    WHERE (pde.description_embedding <=> query_embedding) <= (1 - similarity_threshold)
    ORDER BY similarity_score DESC
    LIMIT max_results;
END;
$$ LANGUAGE plpgsql;
```

## üîå C√°c M·∫´u Qu·∫£n L√Ω K·∫øt N·ªëi

Qu·∫£n l√Ω k·∫øt n·ªëi c∆° s·ªü d·ªØ li·ªáu hi·ªáu qu·∫£ l√† y·∫øu t·ªë quan tr·ªçng ƒë·ªëi v·ªõi hi·ªáu su·∫•t m√°y ch·ªß MCP:

### C·∫•u H√¨nh B·ªô K·∫øt N·ªëi

```python
class ConnectionPoolManager:
    """Manages PostgreSQL connection pools."""
    
    async def create_pool(self) -> Pool:
        """Create optimized connection pool."""
        return await asyncpg.create_pool(
            host=self.config.db_host,
            port=self.config.db_port,
            database=self.config.db_name,
            user=self.config.db_user,
            password=self.config.db_password,
            
            # Pool configuration
            min_size=2,          # Minimum connections
            max_size=10,         # Maximum connections
            max_inactive_connection_lifetime=300,  # 5 minutes
            
            # Query configuration
            command_timeout=30,   # Query timeout
            server_settings={
                "application_name": "zava-mcp-server",
                "jit": "off",          # Disable JIT for stability
                "work_mem": "4MB",     # Limit work memory
                "statement_timeout": "30s"
            }
        )
    
    async def execute_with_retry(
        self, 
        query: str, 
        params: Tuple = None,
        max_retries: int = 3
    ) -> List[Dict[str, Any]]:
        """Execute query with automatic retry logic."""
        
        for attempt in range(max_retries):
            try:
                async with self.pool.acquire() as conn:
                    if params:
                        rows = await conn.fetch(query, *params)
                    else:
                        rows = await conn.fetch(query)
                    return [dict(row) for row in rows]
                    
            except (ConnectionError, InterfaceError) as e:
                if attempt == max_retries - 1:
                    raise
                
                # Exponential backoff
                await asyncio.sleep(2 ** attempt)
                logger.warning(f"Database connection failed, retrying ({attempt + 1}/{max_retries})")
```

### Qu·∫£n L√Ω V√≤ng ƒê·ªùi T√†i Nguy√™n

```python
class MCPServerManager:
    """Manages MCP server lifecycle and resources."""
    
    async def startup(self):
        """Initialize server resources."""
        # Create database connection pool
        self.db_pool = await self.pool_manager.create_pool()
        
        # Initialize AI services
        self.ai_client = await self.create_ai_client()
        
        # Setup monitoring
        self.metrics_collector = MetricsCollector()
        
        logger.info("MCP server startup complete")
    
    async def shutdown(self):
        """Cleanup server resources."""
        try:
            # Close database connections
            if self.db_pool:
                await self.db_pool.close()
            
            # Cleanup AI client
            if self.ai_client:
                await self.ai_client.close()
            
            # Flush metrics
            await self.metrics_collector.flush()
            
            logger.info("MCP server shutdown complete")
            
        except Exception as e:
            logger.error(f"Error during shutdown: {e}")
    
    async def health_check(self) -> Dict[str, str]:
        """Verify server health status."""
        status = {}
        
        # Check database connection
        try:
            async with self.db_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            status["database"] = "healthy"
        except Exception as e:
            status["database"] = f"unhealthy: {e}"
        
        # Check AI service
        try:
            await self.ai_client.health_check()
            status["ai_service"] = "healthy"
        except Exception as e:
            status["ai_service"] = f"unhealthy: {e}"
        
        return status
```

## üõ°Ô∏è C√°c M·∫´u X·ª≠ L√Ω L·ªói v√† TƒÉng C∆∞·ªùng ƒê·ªô B·ªÅn

X·ª≠ l√Ω l·ªói m·∫°nh m·∫Ω ƒë·∫£m b·∫£o ho·∫°t ƒë·ªông ƒë√°ng tin c·∫≠y c·ªßa m√°y ch·ªß MCP:

### C√°c Lo·∫°i L·ªói Ph√¢n C·∫•p

```python
class MCPError(Exception):
    """Base MCP server error."""
    def __init__(self, message: str, error_code: str = "MCP_ERROR"):
        self.message = message
        self.error_code = error_code
        super().__init__(message)

class DatabaseError(MCPError):
    """Database operation errors."""
    def __init__(self, message: str, query: str = None):
        super().__init__(message, "DATABASE_ERROR")
        self.query = query

class AuthorizationError(MCPError):
    """Access control errors."""
    def __init__(self, message: str, user_id: str = None):
        super().__init__(message, "AUTHORIZATION_ERROR")
        self.user_id = user_id

class QueryTimeoutError(DatabaseError):
    """Query execution timeout."""
    def __init__(self, query: str):
        super().__init__(f"Query timeout: {query[:100]}...", query)
        self.error_code = "QUERY_TIMEOUT"

class ValidationError(MCPError):
    """Input validation errors."""
    def __init__(self, field: str, value: Any, constraint: str):
        message = f"Validation failed for {field}: {constraint}"
        super().__init__(message, "VALIDATION_ERROR")
        self.field = field
        self.value = value
```

### Middleware X·ª≠ L√Ω L·ªói

```python
@contextmanager
async def error_handling_context(operation_name: str, user_id: str = None):
    """Centralized error handling for operations."""
    start_time = time.time()
    
    try:
        yield
        
        # Success metrics
        duration = time.time() - start_time
        metrics.operation_success.labels(operation=operation_name).inc()
        metrics.operation_duration.labels(operation=operation_name).observe(duration)
        
    except ValidationError as e:
        logger.warning(f"Validation error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "validation",
            "field": e.field
        })
        metrics.operation_error.labels(operation=operation_name, type="validation").inc()
        raise
        
    except AuthorizationError as e:
        logger.warning(f"Authorization error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "authorization"
        })
        metrics.operation_error.labels(operation=operation_name, type="authorization").inc()
        raise
        
    except DatabaseError as e:
        logger.error(f"Database error in {operation_name}: {e.message}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "database",
            "query": e.query[:100] if e.query else None
        })
        metrics.operation_error.labels(operation=operation_name, type="database").inc()
        raise
        
    except Exception as e:
        logger.error(f"Unexpected error in {operation_name}: {str(e)}", extra={
            "operation": operation_name,
            "user_id": user_id,
            "error_type": "unexpected"
        }, exc_info=True)
        metrics.operation_error.labels(operation=operation_name, type="unexpected").inc()
        raise MCPError(f"Internal server error in {operation_name}")
```

## üìä Chi·∫øn L∆∞·ª£c T·ªëi ∆Øu H√≥a Hi·ªáu Su·∫•t

### Gi√°m S√°t Hi·ªáu Su·∫•t Truy V·∫•n

```python
class QueryPerformanceMonitor:
    """Monitor and optimize query performance."""
    
    def __init__(self):
        self.slow_query_threshold = 1.0  # seconds
        self.query_stats = defaultdict(list)
    
    @contextmanager
    async def monitor_query(self, query: str, operation_type: str = "unknown"):
        """Monitor query execution time and performance."""
        start_time = time.time()
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        
        try:
            yield
            
            duration = time.time() - start_time
            
            # Record performance metrics
            self.query_stats[operation_type].append(duration)
            
            # Log slow queries
            if duration > self.slow_query_threshold:
                logger.warning(f"Slow query detected", extra={
                    "query_hash": query_hash,
                    "duration": duration,
                    "operation_type": operation_type,
                    "query": query[:200]
                })
            
            # Update metrics
            metrics.query_duration.labels(type=operation_type).observe(duration)
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"Query failed", extra={
                "query_hash": query_hash,
                "duration": duration,
                "operation_type": operation_type,
                "error": str(e)
            })
            raise
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Generate performance summary report."""
        summary = {}
        
        for operation_type, durations in self.query_stats.items():
            if durations:
                summary[operation_type] = {
                    "count": len(durations),
                    "avg_duration": sum(durations) / len(durations),
                    "max_duration": max(durations),
                    "min_duration": min(durations),
                    "slow_queries": len([d for d in durations if d > self.slow_query_threshold])
                }
        
        return summary
```

### Chi·∫øn L∆∞·ª£c B·ªô Nh·ªõ ƒê·ªám

```python
class QueryCache:
    """Intelligent query result caching."""
    
    def __init__(self, redis_url: str = None):
        self.cache = {}  # In-memory fallback
        self.redis_client = redis.Redis.from_url(redis_url) if redis_url else None
        self.cache_ttl = 300  # 5 minutes default
    
    async def get_cached_result(
        self, 
        cache_key: str, 
        query_func: Callable,
        ttl: int = None
    ) -> Any:
        """Get result from cache or execute query."""
        ttl = ttl or self.cache_ttl
        
        # Try cache first
        cached_result = await self._get_from_cache(cache_key)
        if cached_result is not None:
            metrics.cache_hit.labels(type="query").inc()
            return cached_result
        
        # Execute query
        metrics.cache_miss.labels(type="query").inc()
        result = await query_func()
        
        # Cache result
        await self._set_in_cache(cache_key, result, ttl)
        
        return result
    
    def _generate_cache_key(self, query: str, user_context: str) -> str:
        """Generate consistent cache key."""
        key_data = f"{query}:{user_context}"
        return hashlib.sha256(key_data.encode()).hexdigest()
```

## üéØ Nh·ªØng ƒêi·ªÉm Ch√≠nh

Sau khi ho√†n th√†nh module n√†y, b·∫°n s·∫Ω hi·ªÉu:

‚úÖ **Ki·∫øn Tr√∫c Ph√¢n L·ªõp**: C√°ch t√°ch bi·ªát c√°c m·ªëi quan t√¢m trong thi·∫øt k·∫ø m√°y ch·ªß MCP  
‚úÖ **M·∫´u C∆° S·ªü D·ªØ Li·ªáu**: Thi·∫øt k·∫ø schema ƒëa kh√°ch h√†ng v√† tri·ªÉn khai RLS  
‚úÖ **Qu·∫£n L√Ω K·∫øt N·ªëi**: B·ªô k·∫øt n·ªëi hi·ªáu qu·∫£ v√† v√≤ng ƒë·ªùi t√†i nguy√™n  
‚úÖ **X·ª≠ L√Ω L·ªói**: C√°c lo·∫°i l·ªói ph√¢n c·∫•p v√† m·∫´u tƒÉng c∆∞·ªùng ƒë·ªô b·ªÅn  
‚úÖ **T·ªëi ∆Øu H√≥a Hi·ªáu Su·∫•t**: Gi√°m s√°t, b·ªô nh·ªõ ƒë·ªám, v√† t·ªëi ∆∞u h√≥a truy v·∫•n  
‚úÖ **S·∫µn S√†ng S·∫£n Xu·∫•t**: C√°c m·ªëi quan t√¢m v·ªÅ h·∫° t·∫ßng v√† m·∫´u v·∫≠n h√†nh  

## üöÄ Ti·∫øp Theo

Ti·∫øp t·ª•c v·ªõi **[Module 02: B·∫£o M·∫≠t v√† ƒêa Kh√°ch H√†ng](../02-Security/README.md)** ƒë·ªÉ t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ:

- Chi ti·∫øt tri·ªÉn khai B·∫£o M·∫≠t C·∫•p H√†ng (RLS)  
- C√°c m·∫´u x√°c th·ª±c v√† ·ªßy quy·ªÅn  
- Chi·∫øn l∆∞·ª£c c√°ch ly d·ªØ li·ªáu ƒëa kh√°ch h√†ng  
- Ki·ªÉm to√°n b·∫£o m·∫≠t v√† c√°c y·∫øu t·ªë tu√¢n th·ªß  

## üìö T√†i Nguy√™n B·ªï Sung

### C√°c M·∫´u Ki·∫øn Tr√∫c
- [Clean Architecture in Python](https://github.com/cosmic-python/code) - C√°c m·∫´u ki·∫øn tr√∫c cho ·ª©ng d·ª•ng Python  
- [Database Design Patterns](https://en.wikipedia.org/wiki/Database_design) - Nguy√™n t·∫Øc thi·∫øt k·∫ø c∆° s·ªü d·ªØ li·ªáu quan h·ªá  
- [Microservices Patterns](https://microservices.io/patterns/) - C√°c m·∫´u ki·∫øn tr√∫c d·ªãch v·ª•  

### C√°c Ch·ªß ƒê·ªÅ N√¢ng Cao V·ªÅ PostgreSQL
- [PostgreSQL Performance Tuning](https://wiki.postgresql.org/wiki/Performance_Optimization) - H∆∞·ªõng d·∫´n t·ªëi ∆∞u h√≥a c∆° s·ªü d·ªØ li·ªáu  
- [Connection Pooling Best Practices](https://www.postgresql.org/docs/current/runtime-config-connection.html) - Qu·∫£n l√Ω k·∫øt n·ªëi  
- [Query Planning and Optimization](https://www.postgresql.org/docs/current/planner-optimizer.html) - Hi·ªáu su·∫•t truy v·∫•n  

### C√°c M·∫´u Async Python
- [AsyncIO Best Practices](https://docs.python.org/3/library/asyncio.html) - C√°c m·∫´u l·∫≠p tr√¨nh async  
- [FastAPI Architecture](https://fastapi.tiangolo.com/advanced/) - Ki·∫øn tr√∫c web Python hi·ªán ƒë·∫°i  
- [Pydantic Models](https://pydantic-docs.helpmanual.io/) - X√°c th·ª±c v√† tu·∫ßn t·ª± h√≥a d·ªØ li·ªáu  

---

**Ti·∫øp Theo**: S·∫µn s√†ng kh√°m ph√° c√°c m·∫´u b·∫£o m·∫≠t? Ti·∫øp t·ª•c v·ªõi [Module 02: B·∫£o M·∫≠t v√† ƒêa Kh√°ch H√†ng](../02-Security/README.md)

---

**Tuy√™n b·ªë mi·ªÖn tr·ª´ tr√°ch nhi·ªám**:  
T√†i li·ªáu n√†y ƒë√£ ƒë∆∞·ª£c d·ªãch b·∫±ng d·ªãch v·ª• d·ªãch thu·∫≠t AI [Co-op Translator](https://github.com/Azure/co-op-translator). M·∫∑c d√π ch√∫ng t√¥i c·ªë g·∫Øng ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c, xin l∆∞u √Ω r·∫±ng c√°c b·∫£n d·ªãch t·ª± ƒë·ªông c√≥ th·ªÉ ch·ª©a l·ªói ho·∫∑c kh√¥ng ch√≠nh x√°c. T√†i li·ªáu g·ªëc b·∫±ng ng√¥n ng·ªØ b·∫£n ƒë·ªãa n√™n ƒë∆∞·ª£c coi l√† ngu·ªìn th√¥ng tin ch√≠nh th·ª©c. ƒê·ªëi v·ªõi c√°c th√¥ng tin quan tr·ªçng, khuy·∫øn ngh·ªã s·ª≠ d·ª•ng d·ªãch v·ª• d·ªãch thu·∫≠t chuy√™n nghi·ªáp b·ªüi con ng∆∞·ªùi. Ch√∫ng t√¥i kh√¥ng ch·ªãu tr√°ch nhi·ªám v·ªÅ b·∫•t k·ª≥ s·ª± hi·ªÉu l·∫ßm ho·∫∑c di·ªÖn gi·∫£i sai n√†o ph√°t sinh t·ª´ vi·ªác s·ª≠ d·ª•ng b·∫£n d·ªãch n√†y.