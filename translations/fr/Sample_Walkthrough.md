<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c86955fe1dbaafcb4540d8875649e2f5",
  "translation_date": "2025-09-29T16:56:39+00:00",
  "source_file": "Sample_Walkthrough.md",
  "language_code": "fr"
}
-->
# Exemple de serveur MCP et PostgreSQL - Guide complet

## Table des mati√®res
1. [Aper√ßu](../..)
2. [Analyse approfondie de l'architecture](../..)
3. [Construction de la solution](../..)
4. [D√©composition des composants](../..)
5. [Guide de d√©ploiement](../..)
6. [Utilisation de la solution](../..)
7. [Fonctionnalit√©s avanc√©es](../..)
8. [D√©pannage](../..)
9. [Bonnes pratiques](../..)

## Aper√ßu

Ce guide explique comment construire et utiliser un **serveur Model Context Protocol (MCP)** pr√™t pour la production, int√©grant PostgreSQL et les services Azure AI. L'exemple illustre des mod√®les de niveau entreprise, notamment la s√©curit√© au niveau des lignes, la recherche s√©mantique et l'acc√®s aux donn√©es multi-locataires.

### Ce que vous apprendrez
- Comment concevoir un serveur MCP avec int√©gration de base de donn√©es
- Impl√©menter la s√©curit√© au niveau des lignes pour des sc√©narios multi-locataires
- Construire une recherche s√©mantique avec les embeddings Azure OpenAI
- Cr√©er des environnements de d√©veloppement bas√©s sur Docker
- D√©ployer une infrastructure Azure avec des mod√®les Bicep
- Int√©grer VS Code pour des analyses aliment√©es par l'IA

### Technologies utilis√©es
- **Protocole MCP** : Protocole Model Context pour l'int√©gration des outils IA
- **FastMCP** : Framework Python moderne pour serveur MCP
- **PostgreSQL** : Base de donn√©es avec extension pgvector pour la recherche s√©mantique
- **Azure OpenAI** : Embeddings textuels et mod√®les GPT optionnels
- **Docker** : Conteneurisation pour des environnements coh√©rents
- **Bicep** : Infrastructure en tant que code pour les ressources Azure
- **VS Code** : Environnement de d√©veloppement avec int√©gration MCP

## üìö Guide d'apprentissage structur√© : /walkthrough

En compl√©ment de ce guide technique, ce d√©p√¥t inclut un **guide d'apprentissage en 12 modules** situ√© dans le r√©pertoire `/walkthrough`. Cette approche structur√©e d√©compose l'impl√©mentation complexe en modules d'apprentissage digestes, parfaits pour les d√©veloppeurs souhaitant comprendre chaque composant √©tape par √©tape.

### Aper√ßu des modules d'apprentissage

| Module | Sujet | Focus | Dur√©e |
|--------|-------|-------|-------|
| **[00-Introduction](walkthrough/00-Introduction/README.md)** | Fondamentaux MCP | Concepts cl√©s, √©tude de cas Zava Retail, aper√ßu de l'architecture | 30 min |
| **[01-Architecture](walkthrough/01-Architecture/README.md)** | Conception syst√®me | Architecture technique, mod√®les de conception, relations entre composants | 45 min |
| **[02-S√©curit√©](walkthrough/02-Security/README.md)** | S√©curit√© d'entreprise | Authentification Azure, s√©curit√© au niveau des lignes, isolation multi-locataires | 60 min |
| **[03-Configuration](walkthrough/03-Setup/README.md)** | Configuration de l'environnement | Configuration Docker, CLI Azure, initialisation du projet | 45 min |
| **[04-Base de donn√©es](walkthrough/04-Database/README.md)** | Couche de donn√©es | Sch√©ma PostgreSQL, configuration pgvector, politiques RLS, donn√©es d'exemple | 60 min |
| **[05-Serveur MCP](walkthrough/05-MCP-Server/README.md)** | Impl√©mentation principale | Framework FastMCP, int√©gration de base de donn√©es, d√©veloppement d'outils | 90 min |
| **[06-Outils](walkthrough/06-Tools/README.md)** | D√©veloppement d'outils | Cr√©ation d'outils MCP, validation de requ√™tes, intelligence d'affaires | 75 min |
| **[07-Recherche s√©mantique](walkthrough/07-Semantic-Search/README.md)** | Int√©gration IA | Embeddings Azure OpenAI, recherche vectorielle, requ√™tes hybrides | 60 min |
| **[08-Test](walkthrough/08-Testing/README.md)** | Assurance qualit√© | Strat√©gies de test, techniques de d√©bogage, tests de performance | 75 min |
| **[09-VS Code](walkthrough/09-VS-Code/README.md)** | Exp√©rience de d√©veloppement | Configuration VS Code, int√©gration Chat IA, workflows de d√©bogage | 45 min |
| **[10-D√©ploiement](walkthrough/10-Deployment/README.md)** | D√©ploiement en production | Conteneurisation, Azure Container Apps, pipelines CI/CD | 90 min |
| **[11-Surveillance](walkthrough/11-Monitoring/README.md)** | Observabilit√© | Application Insights, journalisation structur√©e, m√©triques de performance | 60 min |
| **[12-Bonnes pratiques](walkthrough/12-Best-Practices/README.md)** | Excellence en production | Renforcement de la s√©curit√©, optimisation, mod√®les d'entreprise | 45 min |

### Comment utiliser le guide d'apprentissage

**üìñ Pour apprendre** : Les modules `/walkthrough` fournissent des instructions √©tape par √©tape avec des explications sur la conception de chaque composant. Commencez par le module 00 et progressez dans l'ordre.

**üîß Pour impl√©menter** : Ce fichier Sample_Walkthrough.md offre une analyse technique approfondie et une d√©composition du code pour les d√©veloppeurs souhaitant comprendre rapidement l'impl√©mentation compl√®te.

**üöÄ Pour la production** : Les modules 02, 10, 11 et 12 se concentrent sp√©cifiquement sur le d√©ploiement pr√™t pour la production, la s√©curit√© et les consid√©rations de surveillance.

**üìö Parcours d'apprentissage complet** : Consultez **[/walkthrough/README.md](walkthrough/README.md)** pour une vue d'ensemble compl√®te du guide d'apprentissage avec des objectifs d'apprentissage d√©taill√©s et des pr√©requis.

---

## Analyse approfondie de l'architecture

### Architecture de haut niveau

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   VS Code AI    ‚îÇ    ‚îÇ   MCP Server    ‚îÇ    ‚îÇ   PostgreSQL    ‚îÇ
‚îÇ     Client      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ  (FastMCP)      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   + pgvector    ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ  Azure OpenAI   ‚îÇ
                       ‚îÇ   Embeddings    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Composants principaux

#### 1. **Serveur MCP (`sales_analysis.py`)**
- **Framework FastMCP** : G√®re la communication HTTP/SSE
- **Enregistrement des outils** : Expose les outils de requ√™te et de sch√©ma de base de donn√©es
- **Contexte des requ√™tes** : G√®re l'identification des utilisateurs pour RLS
- **Gestion des erreurs** : Gestion robuste des erreurs et journalisation

#### 2. **Couche base de donn√©es (`sales_analysis_postgres.py`)**
- **Pooling de connexions** : Gestion efficace des connexions asyncpg
- **Fournisseur de sch√©ma** : D√©couverte dynamique des sch√©mas de table
- **Ex√©cution des requ√™tes** : Ex√©cution SQL s√©curis√©e avec RLS
- **Recherche s√©mantique** : Recherche de similarit√© vectorielle avec pgvector

#### 3. **Gestion de la configuration (`config.py`)**
- **Variables d'environnement** : Gestion centralis√©e de la configuration
- **Param√®tres de connexion** : Configuration des services de base de donn√©es et Azure
- **Validation** : Validation des variables d'environnement requises

#### 4. **Infrastructure (`infra/`)**
- **Mod√®les Bicep** : Provisionnement d√©claratif des ressources Azure
- **D√©ploiement de mod√®les** : D√©ploiement automatis√© des mod√®les IA
- **Attributions de r√¥les** : Configuration des r√¥les de s√©curit√©

### Flux de donn√©es

```
1. VS Code AI Client sends query
2. MCP Server receives request with RLS headers
3. Server extracts user identity and sets context
4. Database queries execute with RLS filtering
5. Results return through MCP protocol
6. AI Client processes structured response
```

---

## Construction de la solution

### √âtape 1 : Configuration de la structure du projet

```
project/
‚îú‚îÄ‚îÄ mcp_server/              # MCP server implementation
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Package initialization
‚îÇ   ‚îú‚îÄ‚îÄ sales_analysis.py   # Main MCP server
‚îÇ   ‚îú‚îÄ‚îÄ sales_analysis_postgres.py  # Database layer
‚îÇ   ‚îú‚îÄ‚îÄ sales_analysis_text_embeddings.py  # Semantic search
‚îÇ   ‚îî‚îÄ‚îÄ config.py           # Configuration management
‚îú‚îÄ‚îÄ infra/                  # Infrastructure as Code
‚îÇ   ‚îú‚îÄ‚îÄ main.bicep          # Main deployment template
‚îÇ   ‚îú‚îÄ‚îÄ foundry.bicep       # Azure AI Foundry setup
‚îÇ   ‚îú‚îÄ‚îÄ deploy.ps1          # Windows deployment script
‚îÇ   ‚îî‚îÄ‚îÄ deploy.sh           # Unix deployment script
‚îú‚îÄ‚îÄ data/                   # Database backup and initialization
‚îú‚îÄ‚îÄ docker-init/            # Database initialization scripts
‚îú‚îÄ‚îÄ .vscode/                # VS Code MCP configuration
‚îú‚îÄ‚îÄ docker-compose.yml      # Development environment
‚îú‚îÄ‚îÄ Dockerfile             # MCP server container
‚îî‚îÄ‚îÄ requirements.lock.txt   # Python dependencies
```

### √âtape 2 : D√©pendances principales

**Exigences Python :**
```python
# MCP Framework
mcp[server]>=0.5.0
fastmcp>=0.4.0

# Database Integration
asyncpg>=0.29.0
asyncio-rlock>=0.3.0

# Azure Integration
azure-ai-projects>=1.0.0
azure-identity>=1.19.0
azure-monitor-opentelemetry>=1.7.0

# Data Processing
pydantic>=2.9.0
numpy>=1.24.0

# Development
python-dotenv>=1.0.0
```

**Exigences syst√®me :**
- Docker Desktop pour la conteneurisation
- CLI Azure pour le d√©ploiement
- PostgreSQL avec extension pgvector
- VS Code avec extensions IA

### √âtape 3 : Conception du sch√©ma de base de donn√©es

L'exemple utilise une base de donn√©es de vente au d√©tail avec les tables cl√©s suivantes :

```sql
-- Core business entities
retail.stores          -- Store locations and metadata
retail.customers       -- Customer profiles
retail.categories      -- Product categorization
retail.product_types   -- Product type definitions
retail.products        -- Product catalog
retail.orders          -- Customer orders
retail.order_items     -- Order line items
retail.inventory       -- Stock levels

-- Semantic search support
retail.product_description_embeddings  -- Vector embeddings for products
```

**Impl√©mentation de la s√©curit√© au niveau des lignes (RLS) :**
```sql
-- Enable RLS on tables
ALTER TABLE retail.orders ENABLE ROW LEVEL SECURITY;
ALTER TABLE retail.customers ENABLE ROW LEVEL SECURITY;

-- Create policies based on store association
CREATE POLICY store_policy ON retail.orders
  FOR ALL TO PUBLIC
  USING (store_id = get_user_store_id());
```

---

## D√©composition des composants

### Noyau du serveur MCP (`sales_analysis.py`)

#### Mod√®le d'enregistrement des outils
```python
@mcp.tool()
async def execute_sales_query(
    ctx: Context,
    postgresql_query: Annotated[str, Field(description="A well-formed PostgreSQL query.")],
) -> str:
    """Execute PostgreSQL queries with Row Level Security."""
    rls_user_id = get_rls_user_id(ctx)
    
    try:
        return await db_provider.execute_query(
            postgresql_query, rls_user_id=rls_user_id
        )
    except Exception as e:
        logger.error("Error executing database query: %s", e)
        return f"Error executing database query: {e!s}"
```

**Caract√©ristiques principales :**
- **Annotations de type** : Descriptions de champs Pydantic pour la compr√©hension IA
- **Extraction de contexte** : Identit√© utilisateur √† partir des en-t√™tes HTTP
- **Gestion des erreurs** : √âchecs gracieux avec messages informatifs
- **Journalisation** : Journalisation compl√®te des op√©rations

#### Gestion du contexte des requ√™tes
```python
def get_rls_user_id(ctx: Context) -> str:
    """Extract Row Level Security User ID from request context."""
    rls_user_id = get_header(ctx, "x-rls-user-id")
    if rls_user_id is None:
        # Default to placeholder if not provided
        rls_user_id = "00000000-0000-0000-0000-000000000000"
    return rls_user_id
```

### Couche base de donn√©es (`sales_analysis_postgres.py`)

#### Gestion du pool de connexions
```python
class PostgreSQLSchemaProvider:
    async def create_pool(self) -> None:
        """Create connection pool for better resource management."""
        if self.connection_pool is None:
            config_copy = dict(self.postgres_config)
            existing_server_settings = config_copy.pop("server_settings", {})
            
            merged_server_settings = {
                **existing_server_settings,
                "jit": "off",  # Disable JIT to reduce memory usage
                "work_mem": "4MB",  # Limit work memory per query
                "statement_timeout": "30s",  # 30 second statement timeout
            }
            
            self.connection_pool = await asyncpg.create_pool(
                **config_copy,
                min_size=1,
                max_size=3,  # Conservative pool size
                command_timeout=30,
                server_settings=merged_server_settings,
            )
```

**Mod√®les de conception :**
- **Gestion des ressources** : Gestion appropri√©e du cycle de vie du pool
- **Optimisation des performances** : Param√®tres PostgreSQL optimis√©s
- **R√©cup√©ration des erreurs** : Reconnexion et logique de secours
- **S√©curit√©** : Configuration du contexte RLS par connexion

#### Introspection du sch√©ma
```python
async def get_table_schema(self, table_name: str, rls_user_id: str) -> Dict[str, Any]:
    """Return comprehensive schema information for a table."""
    conn = await self.get_connection()
    
    # Set RLS context
    await conn.execute(
        "SELECT set_config('app.current_rls_user_id', $1, false)", 
        rls_user_id
    )
    
    # Get column information
    columns = await conn.fetch("""
        SELECT column_name, data_type, is_nullable, column_default
        FROM information_schema.columns 
        WHERE table_schema = $1 AND table_name = $2
        ORDER BY ordinal_position
    """, schema_name, table_name)
    
    # Get foreign key relationships
    foreign_keys = await conn.fetch("""
        SELECT kcu.column_name, ccu.table_name AS foreign_table_name
        FROM information_schema.table_constraints tc
        JOIN information_schema.key_column_usage kcu ON ...
    """)
```

### Int√©gration de la recherche s√©mantique

#### G√©n√©ration d'embeddings
```python
class SemanticSearchTextEmbedding:
    def generate_query_embedding(self, query: str) -> Optional[List[float]]:
        """Generate embeddings using Azure OpenAI."""
        try:
            response = self.client.embeddings.create(
                input=[query],
                model=self.deployment_name
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error("Embedding generation failed: %s", e)
            return None
```

#### Recherche de similarit√© vectorielle
```python
async def search_products_by_similarity(
    self,
    query_embedding: List[float],
    rls_user_id: str,
    max_rows: int = 20,
    similarity_threshold: float = 30.0,
) -> str:
    """Search products using pgvector cosine similarity."""
    
    # Convert similarity percentage to distance threshold
    distance_threshold = 1.0 - (similarity_threshold / 100.0)
    
    query = f"""
        SELECT p.*, (pde.description_embedding <=> $1::vector) as distance
        FROM {SCHEMA_NAME}.product_description_embeddings pde
        JOIN {SCHEMA_NAME}.products p ON pde.product_id = p.product_id
        WHERE (pde.description_embedding <=> $1::vector) <= $3
        ORDER BY distance
        LIMIT $2
    """
    
    rows = await conn.fetch(query, embedding_str, max_rows, distance_threshold)
```

---

## Guide de d√©ploiement

### D√©ploiement de l'infrastructure Azure

#### 1. **Structure des mod√®les Bicep**

**Mod√®le principal (`main.bicep`) :**
```bicep
targetScope = 'subscription'

// Core parameters
param resourcePrefix string
param location string
param models array = [
  {
    name: 'text-embedding-3-small'
    format: 'OpenAI'
    version: '1'
    capacity: 50
  }
]

// Deploy foundry and project resources
module foundry 'foundry.bicep' = {
  name: 'foundry-account-deployment'
  scope: rg
  params: {
    foundryResourceName: foundryResourceName
    location: location
  }
}

module foundryProject 'foundry-project.bicep' = {
  name: 'foundry-project-deployment'
  scope: rg
  dependsOn: [foundry]
  params: {
    foundryResourceName: foundry.outputs.accountName
    aiProjectName: aiProjectName
  }
}
```

#### 2. **Automatisation du d√©ploiement**

**D√©ploiement PowerShell (`deploy.ps1`) :**
```powershell
# Generate unique suffix for resources
$UNIQUE_SUFFIX = -join ((97..122) + (48..57) | Get-Random -Count 4 | ForEach-Object { [char]$_ })

# Deploy Azure resources
az deployment sub create `
  --name "$DEPLOYMENT_NAME" `
  --location "$RG_LOCATION" `
  --template-file main.bicep `
  --parameters location="$RG_LOCATION" `
  --parameters resourcePrefix="$RESOURCE_PREFIX" `
  --parameters uniqueSuffix="$UNIQUE_SUFFIX" `
  --parameters models="$modelsJson"

# Create service principal for authentication
$spResult = az ad sp create-for-rbac `
    --name "zava-mcp-server-sp" `
    --role "Cognitive Services OpenAI User" `
    --scopes "/subscriptions/$SubId/resourceGroups/$RESOURCE_GROUP_NAME"

# Generate .env file with configuration
@"
PROJECT_ENDPOINT=$PROJECTS_ENDPOINT
AZURE_OPENAI_ENDPOINT=$AZURE_OPENAI_ENDPOINT
EMBEDDING_MODEL_DEPLOYMENT_NAME="text-embedding-3-small"
AZURE_CLIENT_ID=$clientId
AZURE_CLIENT_SECRET=$clientSecret
AZURE_TENANT_ID=$tenantId
"@ | Out-File -FilePath "../.env"
```

### Configuration du d√©veloppement local

#### 1. **Configuration Docker Compose**
```yaml
# docker-compose.yml
version: '3.8'
services:
  postgres:
    image: pgvector/pgvector:pg17
    environment:
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    volumes:
      - ./data:/backup_data:ro
      - ./docker-init:/docker-entrypoint-initdb.d:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d zava"]
      interval: 15s
      retries: 5

  mcp_server:
    build: .
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "8000:8000"
    env_file:
      - .env
```

#### 2. **Initialisation de la base de donn√©es**
```bash
# docker-init/init-db.sh
#!/bin/bash
set -e

# Create extensions
psql -v ON_ERROR_STOP=1 --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" <<-EOSQL
    CREATE EXTENSION IF NOT EXISTS vector;
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
EOSQL

# Restore database backup
if [ -f /backup_data/zava_retail_2025_07_21_postgres_rls.backup ]; then
    pg_restore --username "$POSTGRES_USER" --dbname "$POSTGRES_DB" \
               --verbose --clean --no-acl --no-owner \
               /backup_data/zava_retail_2025_07_21_postgres_rls.backup
fi
```

---

## Utilisation de la solution

### Int√©gration VS Code

#### 1. **Configuration MCP (`.vscode/mcp.json`)**
```json
{
    "servers": {
        "zava-sales-analysis-headoffice": {
            "url": "http://127.0.0.1:8000/mcp",
            "type": "http",
            "headers": {"x-rls-user-id": "00000000-0000-0000-0000-000000000000"}
        },
        "zava-sales-analysis-seattle": {
            "url": "http://127.0.0.1:8000/mcp",
            "type": "http", 
            "headers": {"x-rls-user-id": "f47ac10b-58cc-4372-a567-0e02b2c3d479"}
        }
    }
}
```

#### 2. **Exemples de requ√™tes**

**D√©couverte de sch√©ma :**
```
AI: #zava What tables are available in the database?
```
*Le serveur MCP utilise `get_multiple_table_schemas` pour retourner les structures de table*

**Analyse des ventes :**
```
AI: #zava Show me the top 10 products by revenue last quarter
```
*G√©n√®re du SQL avec des jointures et un filtrage par date appropri√©s*

**Recherche s√©mantique :**
```
AI: #zava Find products similar to "waterproof electrical connectors"
```
*Utilise des embeddings pour trouver des produits s√©mantiquement similaires*

**Analyse multi-magasins :**
```
# Switch to Seattle store manager context
AI: #zava-seattle What are our best-selling categories this month?
```
*RLS garantit que seules les donn√©es du magasin de Seattle sont accessibles*

### Mod√®les de requ√™tes avanc√©s

#### 1. **Analyse de s√©ries temporelles**
```sql
-- Generated by AI through MCP server
SELECT 
    DATE_TRUNC('month', o.order_date) as month,
    SUM(oi.total_amount) as revenue,
    COUNT(DISTINCT o.order_id) as order_count
FROM retail.orders o
JOIN retail.order_items oi ON o.order_id = oi.order_id
WHERE o.order_date >= CURRENT_DATE - INTERVAL '12 months'
GROUP BY DATE_TRUNC('month', o.order_date)
ORDER BY month;
```

#### 2. **Performance des produits avec cat√©gories**
```sql
-- AI generates complex joins using schema information
SELECT 
    c.category_name,
    pt.type_name,
    COUNT(DISTINCT p.product_id) as product_count,
    SUM(oi.total_amount) as total_revenue,
    AVG(oi.unit_price) as avg_price
FROM retail.products p
JOIN retail.categories c ON p.category_id = c.category_id
JOIN retail.product_types pt ON p.product_type_id = pt.product_type_id
JOIN retail.order_items oi ON p.product_id = oi.product_id
GROUP BY c.category_name, pt.type_name
ORDER BY total_revenue DESC;
```

---

## Fonctionnalit√©s avanc√©es

### Impl√©mentation de la s√©curit√© au niveau des lignes

#### 1. **Cr√©ation de politiques**
```sql
-- Store-based access control
CREATE POLICY customer_store_policy ON retail.customers
  FOR ALL TO PUBLIC
  USING (store_id = get_current_store_id());

CREATE POLICY order_store_policy ON retail.orders  
  FOR ALL TO PUBLIC
  USING (store_id = get_current_store_id());

-- Function to get current user's store
CREATE OR REPLACE FUNCTION get_current_store_id()
RETURNS uuid AS $$
BEGIN
  RETURN current_setting('app.current_rls_user_id')::uuid;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

#### 2. **Configuration du contexte dans le serveur MCP**
```python
async def execute_query(self, sql_query: str, rls_user_id: str) -> str:
    """Execute query with RLS context."""
    conn = await self.get_connection()
    
    # Set RLS context for this connection
    await conn.execute(
        "SELECT set_config('app.current_rls_user_id', $1, false)", 
        rls_user_id
    )
    
    # Execute user query with RLS filtering
    rows = await conn.fetch(sql_query)
    return self.format_results(rows)
```

### Analyse approfondie de la recherche s√©mantique

#### 1. **Pipeline d'embeddings**
```python
# Generate embeddings for product descriptions
async def generate_product_embeddings():
    products = await get_all_products()
    
    for product in products:
        description = f"{product.name} {product.description} {product.category}"
        embedding = embedding_client.generate_embedding(description)
        
        await store_embedding(product.id, embedding)
```

#### 2. **Optimisation de la recherche de similarit√©**
```sql
-- Create vector index for performance
CREATE INDEX idx_product_embeddings_vector 
ON retail.product_description_embeddings 
USING ivfflat (description_embedding vector_cosine_ops);

-- Optimized similarity query
SELECT p.*, 
       (pde.description_embedding <=> $1::vector) as distance,
       (1 - (pde.description_embedding <=> $1::vector)) * 100 as similarity_percent
FROM retail.product_description_embeddings pde
JOIN retail.products p ON pde.product_id = p.product_id
WHERE (pde.description_embedding <=> $1::vector) < 0.7  -- 30% similarity threshold
ORDER BY distance
LIMIT 20;
```

### Surveillance et observabilit√©

#### 1. **Int√©gration Azure Application Insights**
```python
# Configure telemetry
from azure.monitor.opentelemetry import configure_azure_monitor
from opentelemetry.instrumentation.starlette import StarletteInstrumentor

# Enable monitoring if configured
if config.applicationinsights_connection_string:
    configure_azure_monitor(
        connection_string=config.applicationinsights_connection_string
    )
    StarletteInstrumentor().instrument_app(mcp.sse_app())
```

#### 2. **M√©triques et journalisation personnalis√©es**
```python
# Query execution tracking
@contextmanager
async def track_query_execution(query_type: str):
    start_time = time.time()
    try:
        yield
        duration = time.time() - start_time
        logger.info("Query executed", extra={
            "query_type": query_type,
            "duration_ms": duration * 1000,
            "status": "success"
        })
    except Exception as e:
        duration = time.time() - start_time
        logger.error("Query failed", extra={
            "query_type": query_type,
            "duration_ms": duration * 1000,
            "status": "error",
            "error": str(e)
        })
        raise
```

---

## D√©pannage

### Probl√®mes courants et solutions

#### 1. **Probl√®mes de connexion √† la base de donn√©es**
```python
# Connection diagnostics
async def diagnose_connection():
    try:
        pool = await asyncpg.create_pool(**connection_params, min_size=1)
        conn = await pool.acquire()
        result = await conn.fetchval("SELECT 1")
        await pool.release(conn)
        await pool.close()
        return True
    except Exception as e:
        logger.error("Connection failed: %s", e)
        return False
```

**Solutions courantes :**
- V√©rifiez que PostgreSQL est en cours d'ex√©cution : `docker ps`
- V√©rifiez les param√®tres de connexion dans `.env`
- Validez l'existence de la base de donn√©es : `docker exec -it pg17 psql -U postgres -l`
- Testez la connectivit√© r√©seau : `telnet localhost 5432`

#### 2. **Probl√®mes de politiques RLS**
```sql
-- Debug RLS policies
SELECT schemaname, tablename, policyname, cmd, qual 
FROM pg_policies 
WHERE schemaname = 'retail';

-- Check current RLS setting
SELECT current_setting('app.current_rls_user_id');

-- Temporarily disable RLS for debugging
ALTER TABLE retail.orders DISABLE ROW LEVEL SECURITY;
```

#### 3. **Probl√®mes avec le service d'embeddings**
```python
# Test embedding generation
async def test_embeddings():
    try:
        test_text = "waterproof electrical connector"
        embedding = embedding_client.generate_embedding(test_text)
        logger.info("Embedding generated successfully: %d dimensions", len(embedding))
        return True
    except Exception as e:
        logger.error("Embedding test failed: %s", e)
        return False
```

### Optimisation des performances

#### 1. **R√©glage du pool de connexions**
```python
# Optimize for your workload
connection_pool = await asyncpg.create_pool(
    min_size=2,          # Minimum connections
    max_size=10,         # Maximum connections  
    max_inactive_connection_lifetime=300,  # 5 minutes
    command_timeout=30,   # Query timeout
    server_settings={
        "application_name": "mcp-server",
        "work_mem": "4MB",
        "shared_preload_libraries": "pg_stat_statements"
    }
)
```

#### 2. **Optimisation des requ√™tes**
```sql
-- Add indexes for common query patterns
CREATE INDEX idx_orders_store_date 
ON retail.orders (store_id, order_date);

CREATE INDEX idx_order_items_product 
ON retail.order_items (product_id);

-- Analyze query performance
EXPLAIN (ANALYZE, BUFFERS) 
SELECT ... FROM retail.orders o JOIN retail.order_items oi ...;
```

---

## Bonnes pratiques

### Bonnes pratiques de s√©curit√©

#### 1. **Gestion des variables d'environnement**
```bash
# Use strong, unique passwords
POSTGRES_PASSWORD=$(openssl rand -base64 32)

# Rotate service principal credentials regularly
az ad sp credential reset --id $SP_ID --credential-description "Rotated $(date)"

# Use Azure Key Vault in production
az keyvault secret set --vault-name $VAULT_NAME --name "db-password" --value $PASSWORD
```

#### 2. **Directives pour l'impl√©mentation RLS**
- **Refus par d√©faut** : Commencez avec des politiques restrictives
- **Audit r√©gulier** : Surveillez l'efficacit√© des politiques
- **Test approfondi** : Validez les mod√®les d'acc√®s
- **Documentez les politiques** : Maintenez une documentation claire

#### 3. **S√©curit√© r√©seau**
```yaml
# Production docker-compose with network isolation
networks:
  internal:
    driver: bridge
    internal: true
  external:
    driver: bridge

services:
  postgres:
    networks:
      - internal
    # No external ports in production
  
  mcp_server:
    networks:
      - internal
      - external
    ports:
      - "127.0.0.1:8000:8000"  # Bind to localhost only
```

### Bonnes pratiques de d√©veloppement

#### 1. **Mod√®les de gestion des erreurs**
```python
# Structured error responses
class MCPError(Exception):
    def __init__(self, message: str, error_type: str = "general"):
        self.message = message
        self.error_type = error_type
        super().__init__(message)

async def safe_execute_query(query: str, rls_user_id: str) -> str:
    try:
        return await db_provider.execute_query(query, rls_user_id)
    except asyncpg.PostgresError as e:
        logger.error("Database error: %s", e)
        return json.dumps({"error": "Database query failed", "type": "database"})
    except Exception as e:
        logger.error("Unexpected error: %s", e)
        return json.dumps({"error": "Internal server error", "type": "server"})
```

#### 2. **Strat√©gies de test**
```python
# Unit test example
@pytest.mark.asyncio
async def test_rls_isolation():
    """Test that RLS properly isolates store data."""
    
    # Test Seattle store manager
    seattle_results = await db_provider.execute_query(
        "SELECT COUNT(*) FROM retail.orders",
        rls_user_id="f47ac10b-58cc-4372-a567-0e02b2c3d479"
    )
    
    # Test Redmond store manager  
    redmond_results = await db_provider.execute_query(
        "SELECT COUNT(*) FROM retail.orders", 
        rls_user_id="e7f8a9b0-c1d2-3e4f-5678-90abcdef1234"
    )
    
    # Results should be different due to RLS
    assert seattle_results != redmond_results
```

#### 3. **Surveillance et alertes**
```python
# Custom metrics for monitoring
from prometheus_client import Counter, Histogram, start_http_server

query_counter = Counter('mcp_queries_total', 'Total queries executed', ['query_type'])
query_duration = Histogram('mcp_query_duration_seconds', 'Query execution time')

@query_duration.time()
async def execute_query_with_metrics(query: str, rls_user_id: str):
    query_counter.labels(query_type='sales_analysis').inc()
    return await db_provider.execute_query(query, rls_user_id)
```

### Bonnes pratiques de d√©ploiement

#### 1. **Infrastructure en tant que code**
```bicep
// Use parameter files for different environments
param environment string = 'dev'
param location string = 'westus2'

// Apply consistent naming conventions
var resourcePrefix = 'zava-mcp-${environment}'
var resourceGroupName = 'rg-${resourcePrefix}-${uniqueSuffix}'

// Use tags for resource management
var commonTags = {
  Environment: environment
  Project: 'zava-mcp-server'
  ManagedBy: 'bicep'
  CreatedDate: utcNow('yyyy-MM-dd')
}
```

#### 2. **Int√©gration des pipelines CI/CD**
```yaml
# Azure DevOps pipeline example
- task: AzureCLI@2
  displayName: 'Deploy Infrastructure'
  inputs:
    azureSubscription: $(azureServiceConnection)
    scriptType: 'bash'
    scriptLocation: 'inlineScript'
    inlineScript: |
      az deployment sub create \
        --name "mcp-server-$(Build.BuildId)" \
        --location $(location) \
        --template-file infra/main.bicep \
        --parameters environment=$(environment)

- task: Docker@2
  displayName: 'Build and Push MCP Server'
  inputs:
    command: 'buildAndPush'
    repository: 'zava-mcp-server'
    tags: '$(Build.BuildId)'
```

---

Ce guide complet fournit les bases pour construire, d√©ployer et exploiter un serveur MCP pr√™t pour la production avec une int√©gration PostgreSQL. Les mod√®les et pratiques d√©montr√©s ici peuvent √™tre √©tendus √† d'autres domaines et cas d'utilisation tout en maintenant la s√©curit√©, les performances et la maintenabilit√©.

---

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.